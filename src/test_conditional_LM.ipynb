{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "import utils_testing\n",
    "import utils\n",
    "from utils import str2bool\n",
    "import colorama\n",
    "import re\n",
    "import sys\n",
    "from gpt2_model.tokenization_gpt2 import GPT2Tokenizer\n",
    "from gpt2_model.modeling_gpt2_condition import GPT2LMHeadModel\n",
    "from gpt2_model.configuration_gpt2 import GPT2Config\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch Interactive LM')\n",
    "#path\n",
    "parser.add_argument('--checkpoint_topics', type=str, default='../models/',\n",
    "                    help='topical model checkpoint to use')\n",
    "parser.add_argument('--checkpoint_conditional', type=str, default='../models/',\n",
    "                    help='conditional LM model checkpoint to use')\n",
    "parser.add_argument('--emb_file', type=str, default='target_emb.pt',\n",
    "                    help='path to a word embedding file')\n",
    "parser.add_argument('--word_dict', type=str, default='../data/processed/wiki2016_gpt2/tensors_all_min100/dict_idx_compact',\n",
    "                    help='path to a dictionary file')\n",
    "#parser.add_argument('--outf', type=str, default='../gen_log/generated.txt',\n",
    "#                    help='output file for generated text')\n",
    "\n",
    "#parser.add_argument('--batch_size', type=int, default=3, metavar='N',\n",
    "#                    help='batch size')\n",
    "parser.add_argument('--num_sent_gen', type=int, default=3, metavar='N',\n",
    "                    help='In each prompt, generate how many sentences')\n",
    "parser.add_argument('--gen_sent_len', type=int, default=100, metavar='N',\n",
    "                    help='In each prompt, generate sentences with length gen_sent_len')\n",
    "parser.add_argument('--bptt', type=int, default=512,\n",
    "                    help='sequence length')\n",
    "parser.add_argument('--bptt_conditional', type=int, default=256,\n",
    "                    help='sequence length')\n",
    "parser.add_argument('--top_k_nn', type=int, default=5,\n",
    "                    help='Representing each topic using how many words')\n",
    "\n",
    "parser.add_argument('--cuda_topics', type=str2bool, nargs='?', default=True,\n",
    "                    help='use CUDA for topical model')\n",
    "parser.add_argument('--cuda_conditional', type=str2bool, nargs='?', default=True,\n",
    "                    help='use CUDA for conditional LM')\n",
    "parser.add_argument('--single_gpu', default=True, action='store_true',\n",
    "                    help='use single GPU')\n",
    "\n",
    "utils_testing.add_model_arguments(parser)\n",
    "\n",
    "args = parser.parse_args(\"\"\"--checkpoint_topics ../models/future_topic_all-20200106-222318\n",
    "                         --checkpoint_conditional ../models/conditional_all-20200106-235956\n",
    "                         --word_dict ../data/processed/wiki2016_gpt2/tensors_all_min100/dict_idx_compact\"\"\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cuda:1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (linear_future): Linear(in_features=300, out_features=768, bias=True)\n",
       "    (wpe_future): Embedding(1024, 768)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if args.emb_file == \"target_emb.pt\":\n",
    "    args.emb_file =  os.path.join(args.checkpoint_topics,\"target_emb.pt\")\n",
    "device_topics = torch.device(\"cuda:0\" if args.cuda_topics else \"cpu\")\n",
    "device_conditional = torch.device(\"cuda:1\" if args.cuda_conditional else \"cpu\")\n",
    "with open(args.word_dict) as f_in:\n",
    "    idx2word_freq = utils.load_idx2word_freq(f_in)\n",
    "word_d2_idx = {}\n",
    "for i in range(len(idx2word_freq)):\n",
    "    w, freq = idx2word_freq[i]\n",
    "    word_d2_idx[w] = i\n",
    "\n",
    "parallel_encoder, parallel_decoder, encoder, decoder, word_norm_emb = utils.loading_all_models(args, idx2word_freq, device_topics)\n",
    "output_emb_size = word_norm_emb.size(1)\n",
    "print(next(encoder.parameters()).device)\n",
    "\n",
    "model_name = 'gpt2'\n",
    "\n",
    "encoder_state_dict = torch.load(os.path.join(args.checkpoint_conditional, 'encoder.pt'), map_location=device_conditional)\n",
    "gpt2_config = GPT2Config.from_pretrained(model_name)\n",
    "gpt2_config.word_emb_dim = output_emb_size\n",
    "model_condition = GPT2LMHeadModel.from_pretrained(model_name, state_dict = encoder_state_dict, config = gpt2_config).cuda(device_conditional)\n",
    "print(next(model_condition.parameters()).device)\n",
    "\n",
    "tokenizer_GPT2 = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "model_condition.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_future_topics(prompt, encoder, decoder, word_norm_emb, n_basis, top_k, bptt, idx2word_freq, tokenizer_GPT2, device_topics):\n",
    "    tokenized_text = tokenizer_GPT2.tokenize(prompt, add_prefix_space=True)\n",
    "    indexed_tokens = tokenizer_GPT2.convert_tokens_to_ids(tokenized_text)\n",
    "    start_idx = len(indexed_tokens) - bptt\n",
    "    if start_idx > 0:\n",
    "        indexed_tokens = indexed_tokens[start_idx:]\n",
    "    feature = torch.tensor(indexed_tokens, dtype=torch.long, device=device_topics).unsqueeze(0)\n",
    "    output_emb, past = parallel_encoder(feature)\n",
    "    output_emb_last = output_emb[:,-1,:]\n",
    "    basis_pred = decoder(output_emb_last)\n",
    "    basis_norm_pred = basis_pred / (0.000000000001 + basis_pred.norm(dim = 2, keepdim=True) )\n",
    "    \n",
    "    basis_norm_pred = basis_norm_pred.permute(0,2,1)\n",
    "    sim_pairwise = torch.matmul(word_norm_emb.unsqueeze(dim = 0), basis_norm_pred)\n",
    "    top_value, top_index = torch.topk(sim_pairwise, top_k, dim = 1, sorted=True)\n",
    "    top_value = top_value / (0.000000000001 + top_value.sum(dim = 1, keepdim=True) )\n",
    "    #out_str = ''\n",
    "    for j in range(n_basis):\n",
    "        out_str = str(j) + ', '\n",
    "        for k in range(top_k):\n",
    "            word_nn = idx2word_freq[top_index[0,k,j].item()][0]\n",
    "            out_str += word_nn+' {:5.3f} '.format(top_value[0,k,j].item()) \n",
    "        print(out_str)\n",
    "    print()\n",
    "            \n",
    "    return top_value, top_index, feature\n",
    "\n",
    "def conditional_generation(selected_conditions, gen_sent_len, num_sent_gen, word_d2_idx, idx2word_freq, model_condition, word_norm_emb, top_index, top_value, feature, bptt_conditional, tokenizer_GPT2, device_conditional):\n",
    "    word_norm_emb_top = word_norm_emb[top_index,:]\n",
    "    word_norm_emb_w_sum = torch.sum( word_norm_emb_top * top_value.unsqueeze(-1), dim = 1) / top_value.unsqueeze(-1).sum(dim = 1)\n",
    "    word_w_sum_norm = word_norm_emb_w_sum / (0.000000000001 + word_norm_emb_w_sum.norm(dim = -1, keepdim=True))\n",
    "    word_w_sum_norm = word_w_sum_norm.to(device=device_conditional)\n",
    "    selected_topic_idx = []\n",
    "    selected_word_idx = []\n",
    "    for x in selected_conditions:\n",
    "        if isinstance(x, int):\n",
    "            selected_topic_idx.append(x)\n",
    "        else:\n",
    "            if x not in word_d2_idx:\n",
    "                print('Warning: Ignore the word '+x+' because it is too rare')\n",
    "                continue\n",
    "            selected_word_idx.append(word_d2_idx[x])\n",
    "    selected_topic_idx = torch.tensor(np.sort(selected_topic_idx), dtype=torch.long, device = device_conditional)\n",
    "    selected_word_idx = torch.tensor(selected_word_idx, dtype=torch.long, device = device_conditional)\n",
    "    \n",
    "    end_int = feature.size(1)\n",
    "    max_prompt_len = bptt_conditional - gen_sent_len\n",
    "    start_int = 0\n",
    "    if end_int > max_prompt_len:\n",
    "        start_int = end_int - max_prompt_len\n",
    "    insert_loc_list = []\n",
    "    insert_loc_list.append(end_int - 1)\n",
    "    insert_loc_truncated = np.array(insert_loc_list) - start_int\n",
    "    \n",
    "    feature_expanded = feature[0,start_int:end_int].unsqueeze(0).expand(num_sent_gen,end_int - start_int).to(device = device_conditional)\n",
    "    future_emb_chosen_topics = word_w_sum_norm[0, selected_topic_idx,:]\n",
    "    future_emb_chosen_words = word_norm_emb[selected_word_idx,:]\n",
    "    num_selection = future_emb_chosen_topics.size(0) + future_emb_chosen_words.size(0)\n",
    "    future_emb_chosen = torch.cat([future_emb_chosen_topics, future_emb_chosen_words],dim=0).unsqueeze(0).expand(num_sent_gen,num_selection,word_norm_emb.size(-1))\n",
    "    future_emb_chosen_arr = []\n",
    "    future_emb_chosen_arr.append(future_emb_chosen)\n",
    "    truncate_idx = 0\n",
    "    output = utils_testing.sample_seq(model_condition, feature_expanded, insert_loc_truncated[truncate_idx:], future_emb_chosen_arr[truncate_idx:], gen_sent_len, device_conditional)\n",
    "    output_org = utils_testing.sample_seq(model_condition, feature_expanded, None, None, gen_sent_len, device_conditional)\n",
    "    for j in range(num_sent_gen):\n",
    "        generated_sent = tokenizer_GPT2.convert_tokens_to_string( [tokenizer_GPT2._convert_id_to_token(x) for x in output[j, :].tolist()] )\n",
    "        utils_testing.print_sampled_sent(selected_topic_idx.tolist(), generated_sent, top_index[0,:,:], idx2word_freq, sys.stdout, 'conditional '+ str(j), selected_word_idx.tolist())\n",
    "    for j in range(num_sent_gen):\n",
    "        generated_sent_org = tokenizer_GPT2.convert_tokens_to_string( [tokenizer_GPT2._convert_id_to_token(x) for x in output_org[j, :].tolist()] )\n",
    "        utils_testing.print_sampled_sent(selected_topic_idx.tolist(), generated_sent_org, top_index[0,:,:], idx2word_freq, sys.stdout, 'original '+ str(j), selected_word_idx.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, book 0.221 books 0.212 novels 0.192 author 0.190 paperback 0.185 \n",
      "1, Essays 0.203 Perspectives 0.200 Perspective 0.200 Discourse 0.198 Context 0.198 \n",
      "2, University 0.207 faculty 0.204 undergraduate 0.202 university 0.195 doctoral 0.192 \n",
      "3, Reid 0.204 Sen. 0.202 McConnell 0.199 Biden 0.198 O'Donnell 0.198 \n",
      "4, humanity 0.219 life 0.196 spirituality 0.196 societal 0.195 society 0.195 \n",
      "5, 2011 0.208 2010 0.207 2009 0.202 2012 0.197 2008 0.186 \n",
      "6, know 0.206 sure 0.202 want 0.198 really 0.198 think 0.196 \n",
      "7, insistence 0.208 disdain 0.200 dismissive 0.199 patently 0.197 disingenuous 0.196 \n",
      "8, election 0.207 elections 0.202 Democratic 0.199 Republican 0.198 presidential 0.194 \n",
      "9, U.S. 0.215 States 0.214 United 0.193 US 0.192 America 0.187 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Barack Obama writes a new book\"\n",
    "#prompt = \"The magician curses the zombie\"\n",
    "with torch.no_grad():\n",
    "    top_value, top_index, feature = show_future_topics(prompt, parallel_encoder, parallel_decoder, word_norm_emb, args.n_basis, args.top_k_nn, args.bptt, idx2word_freq, tokenizer_GPT2, device_topics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conditional 0:  about his \u001b[31mlife\u001b[0m. His  classic book  is, We are All Alive, a critique of Western attitudes and worldview and a plea for respect for the planet's most beautiful inhabitants. The author describes his book as  an unapologetic, cynical, cynical, and \u001b[31mdismissive\u001b[0m look at Western \u001b[31msociety\u001b[0m that, frankly, has no human culture and no feeling of its own . His book includes the philosophical analyses and insights on the relationships that separate all humans. It provides insights into the social and cultural history of indigenous\n",
      "4 topic: {'life': 1, 'society': 1}\n",
      "7 topic: {'dismissive': 1}\n",
      "\n",
      "conditional 1:  on the United States for his college students. It contains a brief history of the United States, the philosophy of the United States, and more about the policies and practices of the \u001b[31mUniversity\u001b[0m of Wisconsin. Dr. Ryan writes that the book is an academic treatise on history that is  warmly and friendly , and that he is \u001b[31mhappy\u001b[0m with the book, and that  this book will bring together many people  and that  This volume will be an important and useful one in our understanding of the culture that\n",
      "2 topic: {'University': 1}\n",
      "word: {'happy': 1}\n",
      "\n",
      "conditional 2:  called Politics: A Journal of Media Culture, in which he criticizes the overbroad portrayal of American politics, a liberal \u001b[31msociety\u001b[0m, and the unfashionable position that the average person does in the political economy as a whole. According to his book, Obama is an overawed and cynical individual who is still \u001b[31mhappy\u001b[0m to do a lot of things he considers necessary, so he's making politics more efficient. It says that Obama's actions and activities have led him to fail many times since his presidency. On\n",
      "4 topic: {'society': 1}\n",
      "word: {'happy': 1}\n",
      "\n",
      "original 0:  titled  America Needs Justice. In 2008, she published a new book titled  Our Justice  about the 2008 American Presidential Election: Justice for Justice. As the author of this book, she calls for legal reforms that will address problems that arise in a country that is in a highly disadvantaged economic and political environment. In the book, she describes the role that legal reforms play in trying to prevent poverty alleviation, improving the safety of living, and helping families achieve their full-time fulfillment of their needs\n",
      "\n",
      "original 1:  titled What Do You Want Us To Do? which challenges traditional conservative politicians to decide how their country will work. He also tries to find a way to address those concerns. His book, What Do We Want Us To Do, was released by The Church Publishing Company on April 26, 2015, containing his comments on political issues, on politics talk shows, his work, video comment pieces and interviews with American politics, and his  I Don't believe in politics because I believe my country should be made a\n",
      "\n",
      "original 2:  for the Harvard Law School entitled  Obama: A Life . The book will mark the 40th anniversary of the Clinton administration. He also writes an article for Harvard Law on the  Obama's Election Law . In 2014, he is author, co-editor, and co-editing the Harvard Law Review. The Review received media coverage when it was criticized for allowing Obama to become President of the United States on his 17th birthday after being sworn-in. In the article, he argues that Obama\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected_conditions = [4,7,2,'happy'] #['zombie'] #[2] #['zombie'] #[4,8,2,'happy']\n",
    "gen_sent_len = args.gen_sent_len\n",
    "#gen_sent_len = 100\n",
    "num_sent_gen = args.num_sent_gen\n",
    "with torch.no_grad():\n",
    "    conditional_generation(selected_conditions, gen_sent_len, num_sent_gen, word_d2_idx, idx2word_freq, model_condition, word_norm_emb, top_index, top_value, feature, args.bptt_conditional, tokenizer_GPT2, device_conditional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
