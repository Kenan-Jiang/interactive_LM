{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "import utils_testing\n",
    "import utils\n",
    "from utils import str2bool\n",
    "import colorama\n",
    "import re\n",
    "import sys\n",
    "from gpt2_model.tokenization_gpt2 import GPT2Tokenizer\n",
    "from gpt2_model.modeling_gpt2_condition import GPT2LMHeadModel\n",
    "from gpt2_model.configuration_gpt2 import GPT2Config\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch Interactive LM')\n",
    "#path\n",
    "parser.add_argument('--checkpoint_topics', type=str, default='../models/',\n",
    "                    help='topical model checkpoint to use')\n",
    "parser.add_argument('--checkpoint_conditional', type=str, default='../models/',\n",
    "                    help='conditional LM model checkpoint to use')\n",
    "parser.add_argument('--emb_file', type=str, default='target_emb.pt',\n",
    "                    help='path to a word embedding file')\n",
    "parser.add_argument('--word_dict', type=str, default='../data/processed/wiki2016_gpt2/tensors_all_min100/dict_idx_compact',\n",
    "                    help='path to a dictionary file')\n",
    "#parser.add_argument('--outf', type=str, default='../gen_log/generated.txt',\n",
    "#                    help='output file for generated text')\n",
    "\n",
    "#parser.add_argument('--batch_size', type=int, default=3, metavar='N',\n",
    "#                    help='batch size')\n",
    "parser.add_argument('--num_sent_gen', type=int, default=3, metavar='N',\n",
    "                    help='In each prompt, generate how many sentences')\n",
    "parser.add_argument('--gen_sent_len', type=int, default=50, metavar='N',\n",
    "                    help='In each prompt, generate sentences with length gen_sent_len')\n",
    "parser.add_argument('--bptt', type=int, default=512,\n",
    "                    help='sequence length')\n",
    "parser.add_argument('--bptt_conditional', type=int, default=256,\n",
    "                    help='sequence length')\n",
    "parser.add_argument('--top_k_nn', type=int, default=5,\n",
    "                    help='Representing each topic using how many words')\n",
    "\n",
    "parser.add_argument('--cuda_topics', type=str2bool, nargs='?', default=True,\n",
    "                    help='use CUDA for topical model')\n",
    "parser.add_argument('--cuda_conditional', type=str2bool, nargs='?', default=True,\n",
    "                    help='use CUDA for conditional LM')\n",
    "parser.add_argument('--single_gpu', default=True, action='store_true',\n",
    "                    help='use single GPU')\n",
    "\n",
    "utils_testing.add_model_arguments(parser)\n",
    "\n",
    "args = parser.parse_args(\"\"\"--checkpoint_topics ../models/future_topic_all-20200106-222318\n",
    "                         --checkpoint_conditional ../models/conditional_all-20200106-235956\n",
    "                         --word_dict ../data/processed/wiki2016_gpt2/tensors_all_min100/dict_idx_compact\"\"\".split())\n",
    "\n",
    "#new average model\n",
    "args = parser.parse_args(\"\"\"--checkpoint_topics ../models/future_topic_all-20200106-222318\n",
    "                         --checkpoint_conditional ../models/conditional_all-20200115-160129\n",
    "                         --word_dict ../data/processed/wiki2016_gpt2/tensors_all_min100/dict_idx_compact\"\"\".split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (linear_future): Linear(in_features=300, out_features=768, bias=True)\n",
       "    (wpe_future): Embedding(1024, 768)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if args.emb_file == \"target_emb.pt\":\n",
    "    args.emb_file =  os.path.join(args.checkpoint_topics,\"target_emb.pt\")\n",
    "device_topics = torch.device(\"cuda:0\" if args.cuda_topics else \"cpu\")\n",
    "device_conditional = torch.device(\"cuda:1\" if args.cuda_conditional else \"cpu\")\n",
    "with open(args.word_dict) as f_in:\n",
    "    idx2word_freq = utils.load_idx2word_freq(f_in)\n",
    "word_d2_idx = {}\n",
    "for i in range(len(idx2word_freq)):\n",
    "    w, freq = idx2word_freq[i]\n",
    "    word_d2_idx[w] = i\n",
    "\n",
    "parallel_encoder, parallel_decoder, encoder, decoder, word_norm_emb = utils.loading_all_models(args, idx2word_freq, device_topics)\n",
    "output_emb_size = word_norm_emb.size(1)\n",
    "#print(next(encoder.parameters()).device)\n",
    "\n",
    "model_name = 'gpt2'\n",
    "\n",
    "encoder_state_dict = torch.load(os.path.join(args.checkpoint_conditional, 'encoder.pt'), map_location=device_conditional)\n",
    "gpt2_config = GPT2Config.from_pretrained(model_name)\n",
    "gpt2_config.word_emb_dim = output_emb_size\n",
    "model_condition = GPT2LMHeadModel.from_pretrained(model_name, state_dict = encoder_state_dict, config = gpt2_config).cuda(device_conditional)\n",
    "#print(next(model_condition.parameters()).device)\n",
    "\n",
    "tokenizer_GPT2 = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "model_condition.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_future_topics(prompt, encoder, decoder, word_norm_emb, n_basis, top_k, bptt, idx2word_freq, tokenizer_GPT2, device_topics):\n",
    "    tokenized_text = tokenizer_GPT2.tokenize(prompt, add_prefix_space=True)\n",
    "    print(tokenized_text)\n",
    "    indexed_tokens = tokenizer_GPT2.convert_tokens_to_ids(tokenized_text)\n",
    "    start_idx = len(indexed_tokens) - bptt\n",
    "    if start_idx > 0:\n",
    "        indexed_tokens = indexed_tokens[start_idx:]\n",
    "    feature = torch.tensor(indexed_tokens, dtype=torch.long, device=device_topics).unsqueeze(0)\n",
    "    output_emb, past = parallel_encoder(feature)\n",
    "    output_emb_last = output_emb[:,-1,:]\n",
    "    basis_pred = decoder(output_emb_last)\n",
    "    basis_norm_pred = basis_pred / (0.000000000001 + basis_pred.norm(dim = 2, keepdim=True) )\n",
    "    \n",
    "    basis_norm_pred = basis_norm_pred.permute(0,2,1)\n",
    "    sim_pairwise = torch.matmul(word_norm_emb.unsqueeze(dim = 0), basis_norm_pred)\n",
    "    top_value, top_index = torch.topk(sim_pairwise, top_k, dim = 1, sorted=True)\n",
    "    top_value = top_value / (0.000000000001 + top_value.sum(dim = 1, keepdim=True) )\n",
    "    #out_str = ''\n",
    "    for j in range(n_basis):\n",
    "        out_str = str(j) + ', '\n",
    "        for k in range(top_k):\n",
    "        #for k in range(3):\n",
    "            word_nn = idx2word_freq[top_index[0,k,j].item()][0]\n",
    "            #out_str += word_nn+' {:5.3f} '.format(top_value[0,k,j].item()) \n",
    "            out_str += word_nn+', ' \n",
    "        print(out_str)\n",
    "    print()\n",
    "            \n",
    "    return top_value, top_index, feature\n",
    "\n",
    "def conditional_generation(selected_conditions, gen_sent_len, num_sent_gen, word_d2_idx, idx2word_freq, model_condition, word_norm_emb, top_index, top_value, feature, bptt_conditional, tokenizer_GPT2, device_conditional):\n",
    "    word_norm_emb_top = word_norm_emb[top_index,:]\n",
    "    word_norm_emb_w_sum = torch.sum( word_norm_emb_top * top_value.unsqueeze(-1), dim = 1) / top_value.unsqueeze(-1).sum(dim = 1)\n",
    "    word_w_sum_norm = word_norm_emb_w_sum / (0.000000000001 + word_norm_emb_w_sum.norm(dim = -1, keepdim=True))\n",
    "    word_w_sum_norm = word_w_sum_norm.to(device=device_conditional)\n",
    "    selected_topic_idx = []\n",
    "    selected_word_idx = []\n",
    "    for x in selected_conditions:\n",
    "        if isinstance(x, int):\n",
    "            selected_topic_idx.append(x)\n",
    "        else:\n",
    "            if x not in word_d2_idx:\n",
    "                print('Warning: Ignore the word '+x+' because it is too rare')\n",
    "                continue\n",
    "            selected_word_idx.append(word_d2_idx[x])\n",
    "    selected_topic_idx = torch.tensor(np.sort(selected_topic_idx), dtype=torch.long, device = device_conditional)\n",
    "    selected_word_idx = torch.tensor(selected_word_idx, dtype=torch.long, device = device_conditional)\n",
    "    \n",
    "    end_int = feature.size(1)\n",
    "    max_prompt_len = bptt_conditional - gen_sent_len\n",
    "    start_int = 0\n",
    "    if end_int > max_prompt_len:\n",
    "        start_int = end_int - max_prompt_len\n",
    "    insert_loc_list = []\n",
    "    insert_loc_list.append(end_int - 1)\n",
    "    insert_loc_truncated = np.array(insert_loc_list) - start_int\n",
    "    \n",
    "    feature_expanded = feature[0,start_int:end_int].unsqueeze(0).expand(num_sent_gen,end_int - start_int).to(device = device_conditional)\n",
    "    future_emb_chosen_topics = word_w_sum_norm[0, selected_topic_idx,:]\n",
    "    future_emb_chosen_words = word_norm_emb[selected_word_idx,:]\n",
    "    num_selection = future_emb_chosen_topics.size(0) + future_emb_chosen_words.size(0)\n",
    "    future_emb_chosen = torch.cat([future_emb_chosen_topics, future_emb_chosen_words],dim=0).unsqueeze(0).expand(num_sent_gen,num_selection,word_norm_emb.size(-1))\n",
    "    future_emb_chosen_arr = []\n",
    "    future_emb_chosen_arr.append(future_emb_chosen)\n",
    "    truncate_idx = 0\n",
    "    output = utils_testing.sample_seq(model_condition, feature_expanded, insert_loc_truncated[truncate_idx:], future_emb_chosen_arr[truncate_idx:], gen_sent_len, device_conditional)\n",
    "    output_org = utils_testing.sample_seq(model_condition, feature_expanded, None, None, gen_sent_len, device_conditional)\n",
    "    for j in range(num_sent_gen):\n",
    "        generated_sent = tokenizer_GPT2.convert_tokens_to_string( [tokenizer_GPT2._convert_id_to_token(x) for x in output[j, :].tolist()] )\n",
    "        utils_testing.print_sampled_sent(selected_topic_idx.tolist(), generated_sent, top_index[0,:,:], idx2word_freq, sys.stdout, 'conditional '+ str(j), selected_word_idx.tolist())\n",
    "    for j in range(num_sent_gen):\n",
    "        generated_sent_org = tokenizer_GPT2.convert_tokens_to_string( [tokenizer_GPT2._convert_id_to_token(x) for x in output_org[j, :].tolist()] )\n",
    "        utils_testing.print_sampled_sent(selected_topic_idx.tolist(), generated_sent_org, top_index[0,:,:], idx2word_freq, sys.stdout, 'original '+ str(j), selected_word_idx.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ĠGoogle', 'Ġannounces', 'Ġa', 'Ġnew', 'Ġproduct']\n",
      "0, development, design, innovation, designing, developing, \n",
      "1, interface, device, controller, configuration, system, \n",
      "2, websites, web, website, site, links, \n",
      "3, released, version, versions, release, releases, \n",
      "4, retailer, retailers, selling, brands, retail, \n",
      "5, 2011, 2010, 2012, 2009, 2008, \n",
      "6, provide, specific, use, allow, utilize, \n",
      "7, implementations, APIs, interoperate, backend, browsers, \n",
      "8, investment, investments, profitability, equity, financial, \n",
      "9, Software, Desktop, Apps, Networking, Windows, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Google announces a new product\"\n",
    "#prompt = \"Barack Obama writes a new book\"\n",
    "#prompt = \"Barack Obama writes a new book on spirituality and the role of religion in society\"\n",
    "#prompt = \"Barack Obama became the first African\"\n",
    "#prompt = \"The magician curses the zombie\"\n",
    "#with torch.no_grad():\n",
    "#    top_value, top_index, feature = show_future_topics(prompt, parallel_encoder, parallel_decoder, word_norm_emb, args.n_basis, args.top_k_nn, args.bptt, idx2word_freq, tokenizer_GPT2, device_topics)\n",
    "#prompt = \"Trump, the leader of the Republican\"\n",
    "top_value, top_index, feature = show_future_topics(prompt, parallel_encoder, parallel_decoder, word_norm_emb, args.n_basis, args.top_k_nn, args.bptt, idx2word_freq, tokenizer_GPT2, device_topics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conditional 0:  head coach to head a professional NFL franchise. Former Philadelphia Eagles head coach Mike Shanahan \u001b[31mresigned\u001b[0m, following an attempt to sign a book signing deal with a \u001b[31mformer\u001b[0m Colts team. A new \u001b[31mstory\u001b[0m has it that one of the youngest players ever signed was a young college\n",
      "6 topic: {'former': 1, 'resigned': 1}\n",
      "word: {'story': 1}\n",
      "\n",
      "conditional 1:  U.S. Senator from Massachusetts to be reelected. The \u001b[31mstory\u001b[0m tells the \u001b[31mstory\u001b[0m of two women who are friends and confidants in the United States Senate. A \u001b[31mformer\u001b[0m friend, Senator Hillary Clinton, also \u001b[31mjoined\u001b[0m the Senate in 2001. In the \u001b[31mstory\u001b[0m\n",
      "6 topic: {'former': 1, 'joined': 1}\n",
      "word: {'story': 3}\n",
      "\n",
      "conditional 2:  to win a general election election     and the first African-American to win election as a Democratic President   The \u001b[31mstory\u001b[0m featured several other  black  and  African American  figures in the Democratic Party, but mostly \u001b[31mretired\u001b[0m or turned over\n",
      "6 topic: {'retired': 1}\n",
      "word: {'story': 1}\n",
      "\n",
      "original 0:  President of that organization. On March 29, 2009, Obama met with many of his \u001b[31mformer\u001b[0m wives and family members to discuss plans to expand his health care plans. Obama responded to a request from his \u001b[31mformer\u001b[0m wife, whose health issues Obama faced in a private\n",
      "6 topic: {'former': 2}\n",
      "\n",
      "original 1:  from that state to be a U.S. Senator from the U.S. since Lee Morgan in 1961. In addition, Obama won the election of the Democratic Party of the Democratic Party. In order to be elected as Vice President of the United\n",
      "\n",
      "original 2:  to win election to a full term as President  Obama won his first term with 56 percent of the vote in a landslide with only a small portion coming from the Know-Nothings. On September 5, 2010, Obama's campaign announced its intent\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#selected_conditions = [4,7,2,'happy'] #[4,8,2,'happy'] #['zombie'] #[2] #['zombie'] #[4,8,2,'happy']\n",
    "#selected_conditions = [9, 8]\n",
    "selected_conditions = [6, 'story']\n",
    "gen_sent_len = args.gen_sent_len\n",
    "#gen_sent_len = 100\n",
    "num_sent_gen = args.num_sent_gen\n",
    "conditional_generation(selected_conditions, gen_sent_len, num_sent_gen, word_d2_idx, idx2word_freq, model_condition, word_norm_emb, top_index, top_value, feature, args.bptt_conditional, tokenizer_GPT2, device_conditional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
